{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import window, from_json, col, expr, to_json, struct, when\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType, StructField, StructType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "checkpoint_dir = current_dir.joinpath('checkpoints')\n",
    "joined_checkpoint_dir = checkpoint_dir.joinpath('joined')\n",
    "\n",
    "if joined_checkpoint_dir.exists():\n",
    "    shutil.rmtree(joined_checkpoint_dir)\n",
    "\n",
    "joined_checkpoint_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters \n",
    "\n",
    "> **TODO:** Change the configuration prameters to the appropriate values for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['kafka.kafka.svc.cluster.local:9092'],\n",
       " 'first_name': 'Josh',\n",
       " 'last_name': 'Greenert',\n",
       " 'client_id': 'GreenertJosh',\n",
       " 'topic_prefix': 'GreenertJosh',\n",
       " 'locations_topic': 'GreenertJosh-locations',\n",
       " 'accelerations_topic': 'GreenertJosh-accelerations',\n",
       " 'joined_topic': 'GreenertJosh-joined'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Josh',\n",
    "    last_name='Greenert'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config['locations_topic'] = '{}-locations'.format(config['topic_prefix'])\n",
    "config['accelerations_topic'] = '{}-accelerations'.format(config['topic_prefix'])\n",
    "config['joined_topic'] = '{}-joined'.format(config['topic_prefix'])\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topic Utility Function\n",
    "\n",
    "The `create_kafka_topic` helps create a Kafka topic based on your configuration settings.  For instance, if your first name is *John* and your last name is *Doe*, `create_kafka_topic('locations')` will create a topic with the name `DoeJohn-locations`.  The function will not create the topic if it already exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic \"GreenertJosh-joined\" already exists\n"
     ]
    }
   ],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "    \n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers, \n",
    "        client_id=client_id\n",
    "    )\n",
    "    \n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Created topic \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))\n",
    "\n",
    "create_kafka_topic('joined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** This code is identical to the code used in 9.1 to publish acceleration and location data to the `LastnameFirstname-simple` topic. You will need to add in the code you used to create the `df_accelerations` dataframe. In order to read data from this topic, make sure that you are running the notebook you created in assignment 8 that publishes acceleration and location data to the LastnameFirstname-simple topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/14 01:41:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/14 01:41:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/14 01:41:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Assignment09\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_locations = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\") \\\n",
    "  .option(\"subscribe\", config['locations_topic']) \\\n",
    "  .load()\n",
    "\n",
    "## TODO: Add code to create the df_accelerations dataframe\n",
    "df_accelerations = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\") \\\n",
    "  .option(\"subscribe\", config['accelerations_topic']) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a Spark schema for location and acceleration data as well as a user-defined function (UDF) for parsing the location and acceleration JSON data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "location_schema = StructType([\n",
    "    StructField('offset', DoubleType(), nullable=True),\n",
    "    StructField('id', StringType(), nullable=True),\n",
    "    StructField('ride_id', StringType(), nullable=True),\n",
    "    StructField('uuid', StringType(), nullable=True),\n",
    "    StructField('course', DoubleType(), nullable=True),\n",
    "    StructField('latitude', DoubleType(), nullable=True),\n",
    "    StructField('longitude', DoubleType(), nullable=True),\n",
    "    StructField('geohash', StringType(), nullable=True),\n",
    "    StructField('speed', DoubleType(), nullable=True),\n",
    "    StructField('accuracy', DoubleType(), nullable=True),\n",
    "])\n",
    "\n",
    "acceleration_schema = StructType([\n",
    "    StructField('offset', DoubleType(), nullable=True),\n",
    "    StructField('id', StringType(), nullable=True),\n",
    "    StructField('ride_id', StringType(), nullable=True),\n",
    "    StructField('uuid', StringType(), nullable=True),\n",
    "    StructField('x', DoubleType(), nullable=True),\n",
    "    StructField('y', DoubleType(), nullable=True),\n",
    "    StructField('z', DoubleType(), nullable=True),\n",
    "])\n",
    "\n",
    "udf_parse_acceleration = udf(lambda x: json.loads(x.decode('utf-8')), acceleration_schema)\n",
    "udf_parse_location = udf(lambda x: json.loads(x.decode('utf-8')), location_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  \n",
    "\n",
    "* Complete the code to create the `accelerationsWithWatermark` dataframe. \n",
    "  * Select the `timestamp` field with the alias `acceleration_timestamp`\n",
    "  * Use the `udf_parse_acceleration` UDF to parse the JSON values\n",
    "  * Select the `ride_id` as `acceleration_ride_id`\n",
    "  * Select the `x`, `y`, and `z` columns\n",
    "  * Use the same watermark timespan used in the `locationsWithWatermark` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locationsWithWatermark = df_locations \\\n",
    "  .select(\n",
    "    col('timestamp').alias('location_timestamp'), \n",
    "    udf_parse_location(df_locations['value']).alias('json_value')\n",
    "   ) \\\n",
    "  .select(\n",
    "    col('location_timestamp'), \n",
    "    col('json_value.ride_id').alias('location_ride_id'),\n",
    "    col('json_value.speed').alias('speed'),\n",
    "    col('json_value.latitude').alias('latitude'),\n",
    "    col('json_value.longitude').alias('longitude'),\n",
    "    col('json_value.geohash').alias('geohash'),\n",
    "    col('json_value.accuracy').alias('accuracy')\n",
    "  ) \\\n",
    " .withWatermark('location_timestamp', \"2 seconds\")\n",
    "\n",
    "accelerationsWithWatermark = df_accelerations \\\n",
    "    .select(\n",
    "        col(\"timestamp\").alias(\"acceleration_timestamp\"),\n",
    "        udf_parse_acceleration(df_accelerations[\"value\"]).alias(\"json_value\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"acceleration_timestamp\"),\n",
    "        col(\"json_value.ride_id\").alias(\"acceleration_ride_id\"),\n",
    "        col(\"json_value.x\"),\n",
    "        col(\"json_value.y\"),\n",
    "        col(\"json_value.z\")\n",
    "    ) \\\n",
    "    .withWatermark(\"acceleration_timestamp\", \"2 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  \n",
    "\n",
    "* Complete the code to create the `df_joined` dataframe. See http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins for additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ride_id: string, timestamp: timestamp, speed: double, latitude: double, longitude: double, geohash: string, accuracy: double, x: double, y: double, z: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined = locationsWithWatermark \\\n",
    "    .join(\n",
    "        accelerationsWithWatermark,\n",
    "        (locationsWithWatermark[\"location_ride_id\"] == accelerationsWithWatermark[\"acceleration_ride_id\"])\n",
    "        & (locationsWithWatermark[\"location_timestamp\"] == accelerationsWithWatermark[\"acceleration_timestamp\"]),\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        locationsWithWatermark[\"location_ride_id\"].alias(\"ride_id\"),\n",
    "        locationsWithWatermark[\"location_timestamp\"].alias(\"timestamp\"),\n",
    "        locationsWithWatermark[\"speed\"],\n",
    "        locationsWithWatermark[\"latitude\"],\n",
    "        locationsWithWatermark[\"longitude\"],\n",
    "        locationsWithWatermark[\"geohash\"],\n",
    "        locationsWithWatermark[\"accuracy\"],\n",
    "        accelerationsWithWatermark[\"x\"],\n",
    "        accelerationsWithWatermark[\"y\"],\n",
    "        accelerationsWithWatermark[\"z\"]\n",
    "    )\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you correctly created the `df_joined` dataframe, you should be able to use the following code to create a streaming query that outputs results to the `LastnameFirstname-joined` topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `location_timestamp` cannot be resolved. Did you mean one of the following? [`timestamp`, `latitude`, `longitude`, `ride_id`, `accuracy`].;\n'Project [ride_id#177, timestamp#178-T2000ms, speed#120, latitude#121, longitude#122, geohash#123, accuracy#124, x#145, y#146, z#147, to_json(struct(ride_id, ride_id#177, location_timestamp, 'location_timestamp, speed, speed#120, latitude, latitude#121, longitude, longitude#122, geohash, geohash#123, accuracy, accuracy#124, acceleration_timestamp, 'acceleration_timestamp, x, x#145, y, y#146, z, z#147), Some(Etc/UTC)) AS value#189]\n+- Project [location_ride_id#119 AS ride_id#177, location_timestamp#114-T2000ms AS timestamp#178-T2000ms, speed#120, latitude#121, longitude#122, geohash#123, accuracy#124, x#145, y#146, z#147]\n   +- Join Inner, ((location_ride_id#119 = acceleration_ride_id#143) AND (location_timestamp#114-T2000ms = acceleration_timestamp#138-T2000ms))\n      :- EventTimeWatermark location_timestamp#114: timestamp, 2 seconds\n      :  +- Project [location_timestamp#114, json_value#116.ride_id AS location_ride_id#119, json_value#116.speed AS speed#120, json_value#116.latitude AS latitude#121, json_value#116.longitude AS longitude#122, json_value#116.geohash AS geohash#123, json_value#116.accuracy AS accuracy#124]\n      :     +- Project [timestamp#12 AS location_timestamp#114, <lambda>(value#8)#115 AS json_value#116]\n      :        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@32b02f2b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@771d93c8, [kafka.bootstrap.servers=kafka.kafka.svc.cluster.local:9092, subscribe=GreenertJosh-locations], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42ccce36,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka.kafka.svc.cluster.local:9092, subscribe -> GreenertJosh-locations),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n      +- EventTimeWatermark acceleration_timestamp#138: timestamp, 2 seconds\n         +- Project [acceleration_timestamp#138, json_value#140.ride_id AS acceleration_ride_id#143, json_value#140.x AS x#145, json_value#140.y AS y#146, json_value#140.z AS z#147]\n            +- Project [timestamp#33 AS acceleration_timestamp#138, <lambda>(value#29)#139 AS json_value#140]\n               +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4f70db13, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3b105f35, [kafka.bootstrap.servers=kafka.kafka.svc.cluster.local:9092, subscribe=GreenertJosh-accelerations], [key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42ccce36,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka.kafka.svc.cluster.local:9092, subscribe -> GreenertJosh-accelerations),None), kafka, [key#21, value#22, topic#23, partition#24, offset#25L, timestamp#26, timestampType#27]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_joined \u001b[38;5;241m=\u001b[39m \u001b[43mdf_joined\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mride_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocation_timestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspeed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeohash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macceleration_timestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     12\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m, col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     ) \\\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(key AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.kafka.svc.cluster.local:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoined_topic\u001b[39m\u001b[38;5;124m'\u001b[39m]) \\\n\u001b[1;32m     19\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(joined_checkpoint_dir)) \\\n\u001b[1;32m     20\u001b[0m   \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     ds_joined\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:4789\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   4785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   4786\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4787\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   4788\u001b[0m     )\n\u001b[0;32m-> 4789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `location_timestamp` cannot be resolved. Did you mean one of the following? [`timestamp`, `latitude`, `longitude`, `ride_id`, `accuracy`].;\n'Project [ride_id#177, timestamp#178-T2000ms, speed#120, latitude#121, longitude#122, geohash#123, accuracy#124, x#145, y#146, z#147, to_json(struct(ride_id, ride_id#177, location_timestamp, 'location_timestamp, speed, speed#120, latitude, latitude#121, longitude, longitude#122, geohash, geohash#123, accuracy, accuracy#124, acceleration_timestamp, 'acceleration_timestamp, x, x#145, y, y#146, z, z#147), Some(Etc/UTC)) AS value#189]\n+- Project [location_ride_id#119 AS ride_id#177, location_timestamp#114-T2000ms AS timestamp#178-T2000ms, speed#120, latitude#121, longitude#122, geohash#123, accuracy#124, x#145, y#146, z#147]\n   +- Join Inner, ((location_ride_id#119 = acceleration_ride_id#143) AND (location_timestamp#114-T2000ms = acceleration_timestamp#138-T2000ms))\n      :- EventTimeWatermark location_timestamp#114: timestamp, 2 seconds\n      :  +- Project [location_timestamp#114, json_value#116.ride_id AS location_ride_id#119, json_value#116.speed AS speed#120, json_value#116.latitude AS latitude#121, json_value#116.longitude AS longitude#122, json_value#116.geohash AS geohash#123, json_value#116.accuracy AS accuracy#124]\n      :     +- Project [timestamp#12 AS location_timestamp#114, <lambda>(value#8)#115 AS json_value#116]\n      :        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@32b02f2b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@771d93c8, [kafka.bootstrap.servers=kafka.kafka.svc.cluster.local:9092, subscribe=GreenertJosh-locations], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42ccce36,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka.kafka.svc.cluster.local:9092, subscribe -> GreenertJosh-locations),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n      +- EventTimeWatermark acceleration_timestamp#138: timestamp, 2 seconds\n         +- Project [acceleration_timestamp#138, json_value#140.ride_id AS acceleration_ride_id#143, json_value#140.x AS x#145, json_value#140.y AS y#146, json_value#140.z AS z#147]\n            +- Project [timestamp#33 AS acceleration_timestamp#138, <lambda>(value#29)#139 AS json_value#140]\n               +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@4f70db13, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3b105f35, [kafka.bootstrap.servers=kafka.kafka.svc.cluster.local:9092, subscribe=GreenertJosh-accelerations], [key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@42ccce36,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka.kafka.svc.cluster.local:9092, subscribe -> GreenertJosh-accelerations),None), kafka, [key#21, value#22, topic#23, partition#24, offset#25L, timestamp#26, timestampType#27]\n"
     ]
    }
   ],
   "source": [
    "ds_joined = df_joined \\\n",
    "  .withColumn(\n",
    "    'value', \n",
    "    to_json(\n",
    "        struct(\n",
    "            'ride_id', 'location_timestamp', 'speed', \n",
    "            'latitude', 'longitude', 'geohash', 'accuracy', \n",
    "            'acceleration_timestamp', 'x', 'y', 'z'\n",
    "        )\n",
    "    )\n",
    "    ).withColumn(\n",
    "     'key', col('ride_id')\n",
    "    ) \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka.kafka.svc.cluster.local:9092\") \\\n",
    "  .option(\"topic\", config['joined_topic']) \\\n",
    "  .option(\"checkpointLocation\", str(joined_checkpoint_dir)) \\\n",
    "  .start()\n",
    "\n",
    "try:\n",
    "    ds_joined.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"STOPPING STREAMING DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
