{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import genson\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Correct the function to use the github resource.\n",
    "def read_jsonl_data():\n",
    "    src_data_path = '../../../data/processed/openflights/routes.jsonl.gz'\n",
    "    with gzip.open(src_data_path, 'rb') as f:\n",
    "        records = [json.loads(line) for line in f.readlines()]\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airline': {'airline_id': 410,\n",
       "  'name': 'Aerocondor',\n",
       "  'alias': 'ANA All Nippon Airways',\n",
       "  'iata': '2B',\n",
       "  'icao': 'ARD',\n",
       "  'callsign': 'AEROCONDOR',\n",
       "  'country': 'Portugal',\n",
       "  'active': True},\n",
       " 'src_airport': {'airport_id': 2965,\n",
       "  'name': 'Sochi International Airport',\n",
       "  'city': 'Sochi',\n",
       "  'country': 'Russia',\n",
       "  'iata': 'AER',\n",
       "  'icao': 'URSS',\n",
       "  'latitude': 43.449902,\n",
       "  'longitude': 39.9566,\n",
       "  'altitude': 89,\n",
       "  'timezone': 3.0,\n",
       "  'dst': 'N',\n",
       "  'tz_id': 'Europe/Moscow',\n",
       "  'type': 'airport',\n",
       "  'source': 'OurAirports'},\n",
       " 'dst_airport': {'airport_id': 2990,\n",
       "  'name': 'Kazan International Airport',\n",
       "  'city': 'Kazan',\n",
       "  'country': 'Russia',\n",
       "  'iata': 'KZN',\n",
       "  'icao': 'UWKD',\n",
       "  'latitude': 55.606201171875,\n",
       "  'longitude': 49.278701782227,\n",
       "  'altitude': 411,\n",
       "  'timezone': 3.0,\n",
       "  'dst': 'N',\n",
       "  'tz_id': 'Europe/Moscow',\n",
       "  'type': 'airport',\n",
       "  'source': 'OurAirports'},\n",
       " 'codeshare': False,\n",
       " 'equipment': ['CR2']}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the data shape.\n",
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    validation_csv_path = \"validation.txt\"\n",
    "    \n",
    "    with open(validation_csv_path, 'w') as f:    \n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                # Use the validate method through jsonschema\n",
    "                jsonschema.validate(record, schema)\n",
    "                pass\n",
    "            except ValidationError as e:\n",
    "                # Write the failed record.\n",
    "                f.write(f\"Record {i} Failed: {str(e)}\\n\")\n",
    "                pass\n",
    "    \n",
    "    # Add the records to the routes-schema.json file.\n",
    "    schema = genson.Schema()\n",
    "    \n",
    "    for record in records:\n",
    "        schema.add_object(record)\n",
    "    \n",
    "    # Convert to dict to ensure map-like structure\n",
    "    json_schema = schema.to_dict()\n",
    "\n",
    "    with open(schema_path, 'w') as f:\n",
    "        json.dump(json_schema, f, indent=2)\n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    \n",
    "    # Load the Avro schema\n",
    "    with open(schema_path, 'r') as f:\n",
    "        parsed_schema = json.load(f)\n",
    "    \n",
    "    # Solution suggested was to remove all none values. Confirmed worked but still not working.\n",
    "    with open(data_path, 'wb') as out:\n",
    "        fastavro.writer(out, parsed_schema, records)\n",
    "\n",
    "    \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = '../../../data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    \n",
    "    # Deleted s3 stuff since it doesn't work.\n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "\n",
    "            # Loop over the lines and save the records, then batch them to the table.\n",
    "            record_batches = []\n",
    "            \n",
    "            for line in f:\n",
    "                record = json.loads(line)\n",
    "                record_batch = pa.record_batch([pa.array([json.loads(line) for line in f.readlines()])], names=['Flights'])\n",
    "                record_batches.append(record_batch)\n",
    "            \n",
    "            # Collect all the record batches into the table.\n",
    "            table = pa.Table.from_batches(record_batches)\n",
    "            \n",
    "    # Write the Parquet table to a file\n",
    "    pq.write_table(table, str(parquet_output_path))\n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if not airport.get('airport_id'):\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "    \n",
    "    # set all values to the object if they are present in the record.\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    if not airline.get('name'):\n",
    "        return None\n",
    "    if not airline.get('airline_id'):\n",
    "        return None\n",
    "    \n",
    "    # Professor provided lines start here:\n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    obj.name = airline.get('name')\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')\n",
    "    # End of professor's code.    \n",
    "    \n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    \n",
    "    # check if airline is found or set, set to false if not.\n",
    "    if airline.get('active'):\n",
    "        obj.active = airline.get('active')\n",
    "    else:\n",
    "        obj.active = False\n",
    "    \n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        \n",
    "        # Start of professor's code:\n",
    "        airline = _airline_to_proto_obj(record.get('airline', {}))\n",
    "        if airline:\n",
    "            route.airline.CopyFrom(airline)\n",
    "        src_airport = _airport_to_proto_obj(record.get('src_airport', {}))\n",
    "        # End professor's code.\n",
    "        \n",
    "        # set the remaining pieces.\n",
    "        if record.get('dst_airport'):\n",
    "            route.dst_airport.CopyFrom(_airport_to_proto_obj(record[\"dst_airport\"]))\n",
    "        if 'codeshare' in record and record['codeshare'] is not None:\n",
    "            route.codeshare = record['codeshare']\n",
    "        else:\n",
    "            route.codeshare = False\n",
    "\n",
    "        if record.get('stops'):\n",
    "            route.stops = record[\"stops\"]\n",
    "        if record.get('equipment'):\n",
    "            route.equipment.append(\"equipment\")\n",
    "        \n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.e Output Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison results saved to: results/comparison.csv\n"
     ]
    }
   ],
   "source": [
    "def get_file_size(file_path):\n",
    "    \"\"\"Get the size of a file in bytes\"\"\"\n",
    "    return os.stat(file_path).st_size\n",
    "\n",
    "def get_gzip_size(filepath):\n",
    "    with open(filepath, 'rb') as f_in:\n",
    "        with gzip.open(filepath + \".gz\", 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "    size = os.stat(filepath + \".gz\").st_size\n",
    "    os.remove(filepath + \".gz\")\n",
    "    return size\n",
    "    \n",
    "def get_snappy_size(filepath):\n",
    "    if not os.path.isfile(filepath +'.snappy'):\n",
    "        with open(filepath, 'rb') as infile:\n",
    "            data = infile.read()\n",
    "            compressed_data = snappy.compress(data)\n",
    "            with open(filepath +'.snappy', 'wb') as outfile:\n",
    "                outfile.write(compressed_data)\n",
    "        size = os.stat(filepath + \".snappy\").st_size\n",
    "        os.remove(filepath + \".snappy\")\n",
    "        return size\n",
    "    return os.stat(filepath + \".snappy\").st_size\n",
    "\n",
    "# File paths\n",
    "avro_file = \"results/routes.avro\"\n",
    "pb_file = \"results/routes.pb\"\n",
    "json_file = \"validation.txt\"\n",
    "parquet_file = \"results/routes.parquet\"\n",
    "output_file = \"results/comparison.csv\"\n",
    "\n",
    "# Add the entries for each set of objects.\n",
    "entries = []\n",
    "\n",
    "entries.append({\"format\" : \"avro file\",\"uncompressed\" : get_file_size(avro_file),\"gzip\" : get_gzip_size(avro_file),\"snappy\" : get_snappy_size(avro_file)})\n",
    "entries.append({\"format\" : \"protocol buffer file\",\"uncompressed\" : get_file_size(pb_file),\"gzip\" : get_gzip_size(pb_file),\"snappy\" : get_snappy_size(pb_file)})\n",
    "entries.append({\"format\" : \"json Schema file\",\"uncompressed\" : get_file_size(json_file),\"gzip\" : get_gzip_size(json_file),\"snappy\" : get_snappy_size(json_file)})\n",
    "entries.append({\"format\" : \"parquet file\",\"uncompressed\" : get_file_size(parquet_file),\"gzip\" : get_gzip_size(parquet_file),\"snappy\" : get_snappy_size(parquet_file)})\n",
    "sizes_df = pd.DataFrame(entries)\n",
    "\n",
    "# Push the sizes to the csv\n",
    "sizes_df.to_csv(output_file, sep=',')\n",
    "\n",
    "print(\"Comparison results saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    for record in records:\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            latitude = src_airport.get('latitude')\n",
    "            longitude = src_airport.get('longitude')\n",
    "            if latitude and longitude:\n",
    "                # use pygeohash.encode() to assign geohashes to the records and complete the hashes list\n",
    "                geohash = pygeohash.encode(latitude, longitude)\n",
    "                record['geohash'] = geohash\n",
    "                hashes.append(geohash)\n",
    "                \n",
    "    hashes.sort()\n",
    "    three_letter = sorted(list(set([entry[:3] for entry in hashes])))\n",
    "    hash_index = {value: [] for value in three_letter}\n",
    "    for record in records:\n",
    "        geohash = record.get('geohash')\n",
    "        if geohash:\n",
    "            hash_index[geohash[:3]].append(record)\n",
    "    for key, values in hash_index.items():\n",
    "        output_dir = geoindex_dir.joinpath(str(key[:1])).joinpath(str(key[:2]))\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        output_path = output_dir.joinpath('{}.jsonl.gz'.format(key))\n",
    "        with gzip.open(output_path, 'w') as f:\n",
    "            json_output = '\\n'.join([json.dumps(value) for value in values])\n",
    "            f.write(json_output.encode('utf-8'))\n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest airport is Eppley Airfield with a distance of 19.55 Km\n"
     ]
    }
   ],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    \n",
    "    # Set a location source.\n",
    "    location_temp = pygeohash.encode(latitude, longitude, precision=5)\n",
    "    \n",
    "    # Set the records file again from the beginning (just in case)\n",
    "    records = read_jsonl_data()\n",
    "    \n",
    "    # Set a new record and distance.\n",
    "    short_distance = 20000000\n",
    "    short_record = {}\n",
    "    \n",
    "    # Loop over the records to find the nearest one based on lat and long.\n",
    "    for record in records:\n",
    "        if record.get('src_airport'):\n",
    "            temp_loc = pygeohash.encode(record[\"src_airport\"][\"latitude\"], record[\"src_airport\"][\"longitude\"], precision=5)\n",
    "            if pygeohash.geohash_approximate_distance(location_temp, temp_loc, check_validity=False) < short_distance:\n",
    "                short_distance = pygeohash.geohash_approximate_distance(location_temp, temp_loc, check_validity=False)\n",
    "                short_record = record\n",
    "\n",
    "    # Print a friendly message to the user.\n",
    "    print(f\"The nearest airport is {short_record['src_airport']['name']} with a distance of {round(short_distance/1000,2)} Km\")\n",
    "\n",
    "# Test the airport search function.\n",
    "airport_search(41.1499988, -95.91779)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
